# for all
* [dive into deep learning](https://d2l.ai/d2l-en.pdf)
* [Self-Aware Personalized Federated Learning](https://openreview.net/pdf?id=EqJ5_hZSqgy)
* [An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale](https://arxiv.org/abs/2010.11929)
* [Attention Is All You Need](https://arxiv.org/abs/1706.03762)
* [FedCLIP: Fast Generalization and Personalization for CLIP in Federated Learning](https://arxiv.org/abs/2302.13485)
* [ns3-fl: Simulating Federated Learning with ns-3](https://cseweb.ucsd.edu/~x1yu/docs/wns32022/ekaireb2022ns3fl.pdf)
* [Inductive biases for deep learning of higher-level cognition](https://royalsocietypublishing.org/doi/10.1098/rspa.2021.0068)
* [Federated Optimization in Heterogeneous Networks](https://arxiv.org/abs/1812.06127)
* [Tackling the Objective Inconsistency Problem in Heterogeneous Federated Optimization](https://arxiv.org/abs/2007.07481)
* [Gorilla: Large Language Model Connected with Massive APIs](https://arxiv.org/pdf/2305.15334.pdf) --c,b
* [Toolformer: Language Models Can Teach Themselves to Use Tools](https://arxiv.org/abs/2302.04761)
* [AutoDistil : FEW-SHOT TASK-AGNOSTIC NEURAL ARCHITECTURE SEARCH FOR DISTILLING LARGE LANGUAGE MODELS](https://arxiv.org/pdf/2201.12507.pdf)
* [ChainsFL: Blockchain-driven Federated Learning from Design to Realization](https://ieeexplore.ieee.org/document/9417299) --c
* [Towards a Secure and Reliable Federated Learning using Blockchain](https://arxiv.org/abs/2201.11311) --c
* [Combining AI and AM - Improving Approximate Matching through Transformer Networks](https://arxiv.org/abs/2208.11367)
* [A Systematic Study and Comprehensive Evaluation of ChatGPT on Benchmark Datasets](https://arxiv.org/abs/2305.18486) --e
* [AlpacaFarm: A Simulation Framework for Methods that Learn from Human Feedback](https://arxiv.org/abs/2305.14387)
* [Adding guardrails to advanced chatbots](https://arxiv.org/abs/2306.07500)
* [Towards Building the Federated GPT: Federated Instruction Tuning](https://arxiv.org/abs/2305.05644)
* [A Primer in BERTology: What we know about how BERT works](https://arxiv.org/abs/2002.12327)
* [BERTology](https://huggingface.co/docs/transformers/bertology)
* [CLUE: A Chinese Language Understanding Evaluation Benchmark](https://www.semanticscholar.org/paper/CLUE%3A-A-Chinese-Language-Understanding-Evaluation-Xu-Zhang/18318b10e7c2dd4ad292208f4399eb1d4dca5768)
* [Judging LLM-as-a-judge with MT-Bench and Chatbot Arena](https://arxiv.org/abs/2306.05685) --e
* [VicunaNER: Zero/Few-shot Named Entity Recognition using Vicuna](https://arxiv.org/abs/2305.03253) --y,l
* [WizardLM: Empowering Large Language Models to Follow Complex Instructions](https://arxiv.org/abs/2304.12244) --b
* [A Survey on Evaluation of Large Language Models](https://arxiv.org/abs/2307.03109) --e
* [Can Public Large Language Models Help Private Cross-device Federated Learning?](https://arxiv.org/pdf/2305.12132.pdf) --z,w
* [PandaLM: An Automatic Evaluation Benchmark for LLM Instruction Tuning Optimization](https://arxiv.org/abs/2306.05087) --e
* [ReAct: Synergizing Reasoning and Acting in Language Models](https://arxiv.org/abs/2210.03629) --c,b
* [LLaMA2](file:///C:/Users/skes2/OneDrive/%E6%96%87%E4%BB%B6/LLaMA2.pdf) --c,b,e
* [Awesome RLHF (RL with Human Feedback)](https://github.com/opendilab/awesome-RLHF)
* [ARB: Advanced Reasoning Benchmark for Large Language Models](https://arxiv.org/abs/2307.13692) --e
* [Universal and Transferable Adversarial Attacks on Aligned Language Models](https://llm-attacks.org/zou2023universal.pdf) --c,b
* [ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators](https://arxiv.org/abs/2003.10555) --y,l
* [BIG-Bench Hard](https://github.com/suzgunmirac/BIG-Bench-Hard), [Challenging BIG-Bench tasks and whether chain-of-thought can solve them](https://arxiv.org/pdf/2210.09261.pdf) --e,w
* [Beyond the Imitation Game: Quantifying and extrapolating the capabilities of language models](https://arxiv.org/abs/2206.04615)
* [MMLU (Massive Multitask Language Understanding)](https://paperswithcode.com/dataset/mmlu)
* [TriviaQA: A Large Scale Distantly Supervised Challenge Dataset for Reading Comprehension](https://arxiv.org/abs/1705.03551)
* [LLaMA-Efficient-Tuning](https://github.com/hiyouga/LLaMA-Efficient-Tuning)
* [Axolotl](https://github.com/OpenAccess-AI-Collective/axolotl)
* [xTuring-Easily build, customize and control your own LLMs](https://github.com/stochasticai/xturing)
* [Trending Research-papers with code](https://paperswithcode.com/)


# Blog
* [LlamaIndex adds private data to large language models](https://techcrunch.com/2023/06/06/llamaindex-adds-private-data-to-large-language-models/)
* [LangChain + Vectara: better together](https://blog.langchain.dev/langchain-vectara-better-together/)
* [Google ÈáãÂá∫ 7 Â†ÇÂÆåÂÖ®ÂÖçË≤ªÁîüÊàêÂºè AI Ë™≤Á®ãÔºÅ‰πüÊúâÊèê‰æõÁµ¶ AI Â∞èÁôΩÁöÑÂÖ•ÈñÄË™≤](https://www.inside.com.tw/article/31843-Google-7-new-no-cost-generative-AI-training-courses)
* [How LLMs Are Transforming Enterprise Applications](https://thenewstack.io/how-llms-are-transforming-enterprise-applications/)
* [Google‚Äôs Generative AI Platform Is Now Available To Everyone](https://www.forbes.com/sites/janakirammsv/2023/06/09/googles-generative-ai-platform-is-now-available-to-everyone/)
* [Building a Truly "Open" OpenAI API Server with Open Models Locally](https://lmsys.org/blog/2023-06-09-api-server/) --c,b
* [Distill Large Vision Models into Smaller, Efficient Models with Autodistill](https://blog.roboflow.com/autodistill/) --e,l
* [OpenAI intros new generative text features while reducing pricing](https://techcrunch.com/2023/06/13/openai-intros-new-generative-text-features-while-reducing-pricing/)
* [A New Tool for the Open Source LLM Developer Stack: Aviary](https://thenewstack.io/a-new-tool-for-the-open-source-llm-developer-stack-aviary/)
* [Knowledge Graphs & LLMs: Fine-Tuning Vs. Retrieval-Augmented Generation](https://medium.com/neo4j/knowledge-graphs-llms-fine-tuning-vs-retrieval-augmented-generation-30e875d63a35)
* [Instruction Tuning with GPT-4](https://instruction-tuning-with-gpt-4.github.io/)
* [vLLM: Easy, Fast, and Cheap LLM Serving with PagedAttention](https://vllm.ai) --c,b
* [Microsoft Researchers Introduce KOSMOS-2: A Multimodal Large Language Model That Can Ground To The Visual World](https://www.marktechpost.com/2023/06/28/microsoft-researchers-introduce-kosmos-2-a-multimodal-large-language-model-that-can-ground-to-the-visual-world/)
* [Tutorial: ChatGPT Over Your Data](https://blog.langchain.dev/tutorial-chatgpt-over-your-data/)
* [GOAT-7B-Community model is the SOTA among the open-source 7B models](https://www.blog.goat.ai/goat-7b-community-tops-among-7b-models/)
* [Llama 2 is here - get it on Hugging Face]( https://huggingface.co/blog/llama2,)
* [Haven - Run LLMs on Your Own Cloud](https://www.ycombinator.com/launches/JAy-haven-run-llms-on-your-own-cloud)
* [Yeager.ai x LangChain: Exploring GenWorlds a Framework for Coordinating AI Agents](https://blog.langchain.dev/exploring-genworlds/)
* [How to Chunk Text Data ‚Äî A Comparative Analysis](https://towardsdatascience.com/how-to-chunk-text-data-a-comparative-analysis-3858c4a0997a)
* [Fine-tuning a Large Language Model using Metaflow, featuring LLaMA and LoRA](https://outerbounds.com/blog/llm-tuning-metaflow/)
* [Dolphin üê¨](https://erichartford.com/dolphin) --e
* [Deepspeed](https://www.deepspeed.ai/) --w,z
